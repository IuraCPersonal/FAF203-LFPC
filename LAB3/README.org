#+TITLE: Laboratory Task #3
#+AUTHOR: FCIM FAF-203 Cius Iurie
#+DESCRIPTION: Lexer
#+STARTUP: showeverything
#+OPTIONS: tex:t

* LABORATORY TASKS

1. Define syntax of your programming languages and compute sample code which consist of 2 functions, first main function and second function which performs some computations. Sample should include call of second function from main function.
2. Define grammar of your programming language. Grammar should include sets of terminal and non-terminal symbols, and rules of derivation.
3. Write a program which will perform tokenization of your sample code, in other words write basic lexer of your programming language.
   
* IMPLEMENTATION

** Defining Our Tokens

The first thing we have to do is to define the tokens our lexer is going to output.

Let’s define our Token data structure. In a new =token.py= package we define our *Token struct*: 

#+begin_src python
class Token:
    def __init__(self, Type, Value):
        self.Type = Type
        self.Value = Value
#+end_src

In the same file we add the *Tokens*:

#+begin_src python
class Tokens:
    def __init__(self):
        self.tokens = {
            # {{ Main Tokens }}
            'ILLEGAL': 'ILLEGAL',
            'EOF'    : 'EOF',

            # {{ Ident and Values }}
            'IDENT'  : 'IDENT',
            'INT'    : 'INT',
            'DOUBLE' : 'DOUBLE',
            'STRING' : 'STRING',
            'CHAR'   : 'CHAR',

            # {{ Operators }}
            'ASSIGN'   : 'ASSIGN',
            'PLUS'     : 'PLUS',
            'MINUS'    : 'MINUS',
            'MULTIPLY' : 'MULTIPLY',
            'DIVIDE'   : 'DIVIDE',
            'LT'       : 'LT',
            'GT'       : 'GT',

            # {{ Delimiters }}
            'SEMICOLON' : 'SEMICOLON',
            'COMMA'     : 'COMMA',
            'LPAREN'    : 'LPAREN',
            'RPAREN'    : 'RPAREN',
            'LBRACE'    : 'LBRACE',
            'RBRACE'    : 'RBRACE',

            # {{ Boolean Operators }}
            'EQ'     : 'EQ',
            'NOT'    : 'NOT',
            'NOT_EQ' : 'NOT_EQ',
            'AND'    : 'AND',
            'OR'     : 'OR',
        }

        self.keywords = {
            'fn'    : 'FUNCTION',
            'let'   : 'LET',
            'true'  : 'TRUE',
            'false' : 'FALSE',
            'if'    : 'IF',
            'else'  : 'ELSE',
            'for'   : 'FOR',
            'while' : 'WHILE',
            'return': 'RETURN',
            'int'   : 'INT',
            'double': 'DOUBLE',
            'string': 'STRING',
            'char'  : 'CHAR',
        }
#+end_src

LookupIdent checks the keywords table to see whether the given identifier is in fact a keyword. If it is, it returns the keyword’s TokenType constant. If it isn’t, we just get back token.IDENT, which is the TokenType for all user-defined identifiers.

#+begin_src python
    def lookup_ident(self, ident):
        if ident in self.keywords:
            return self.keywords[ident]
        return 'IDENT'
#+end_src

** The Lexer

We’re going to write our own lexer. It will take source code as input and output the tokens that represent the source code. It will go through its input and output the next token it recognizes. It doesn’t need to buffer or save tokens, since there will only be one method called =NextToken()=, which will output the next token.

#+begin_src python
from token import *

TOKENS = Tokens()

class Lexer:
    def __init__(self, source):
        self.source = source
        self.position = 0
        self.next_position = 0
        self.ch = ''
        self.__read_char()


    # Checks whether the given argument is a number.    
    def __is_digit(self, ch):
        return str(ch).isnumeric()
    

    # Checks whether the given argument is a letter.
    def __is_char(self, ch):
        return str(ch).isalpha()


    # Gives the next character and advance our position in the input string.
    def __read_char(self):
        if self.next_position >= len(self.source):
            self.ch = 0
        else:
            self.ch = self.source[self.next_position]

        self.position = self.next_position
        self.next_position += 1


    # “peek” ahead in the input and not move around in it.
    def __peek_char(self):
        if self.next_position >= len(self.source):
            return 0
        else:
            return self.source[self.next_position]


    # Reads in an identifier and advances our lexer’s positions until it encounters a non-digit.
    def __read_number(self):
        position = self.position
        while self.__is_digit(self.ch):
            self.__read_char()

        return self.source[position:self.position]


    # Reads in an identifier and advances our lexer’s positions until it encounters a non-letter-character.
    def __read_identifier(self):
        position = self.position
        while self.__is_char(self.ch):
            self.__read_char()

        return self.source[position:self.position]


    # Skips the whitespace character.
    def __eat_whitespace(self):
        try:
            while self.ch.isspace() :
                self.__read_char()
        except AttributeError:
            pass


    # Helps with initializing the tokens.
    def new_token(self, token_type, ch):
        return Token(token_type, str(ch))


    # Look at the current character under
    # examination (l.ch) and return a token depending on which character it is.
    def __next_token(self):
        tok = Token(None, None)

        self.__eat_whitespace()

        # Check the current character under examination.
        match self.ch:
            case 0:
                tok.Value = ''
                tok.Type = TOKENS.tokens['EOF']
            # {{ Operators }}
            case '=':
                if self.__peek_char() == '=':
                    ch = self.ch
                    self.__read_char()
                    tok = Token(TOKENS.tokens['EQ'], ch + self.ch)
                else:
                    tok = self.new_token(TOKENS.tokens['ASSIGN'], self.ch)
            case '+':
                tok = self.new_token(TOKENS.tokens['PLUS'], self.ch)
            case '-':
                tok = self.new_token(TOKENS.tokens['MINUS'], self.ch)
            case '*':
                tok = self.new_token(TOKENS.tokens['MULTIPLY'], self.ch)
            case '/':
                tok = self.new_token(TOKENS.tokens['DIVIDE'], self.ch)
            case '<':
                tok = self.new_token(TOKENS.tokens['LT'], self.ch)
            case '>':
                tok = self.new_token(TOKENS.tokens['GT'], self.ch)
            # {{ Boolean Operators }}
            case '!':
                if self.__peek_char() == '=':
                    ch = self.ch
                    self.__read_char()
                    tok = Token(TOKENS.tokens['NOT_EQ'], ch + self.ch)
                else:
                    tok = self.new_token(TOKENS.tokens['NOT'], self.ch)
            case '&':
                if self.__peek_char() == '&':
                    self.__read_char()
                    tok = Token(TOKENS.tokens['AND'], self.ch + self.ch)
                else:
                    tok = self.new_token(TOKENS.tokens['ILLEGAL'], self.ch)
            case '|':
                if self.__peek_char() == '|':
                    self.__read_char()
                    tok = Token(TOKENS.tokens['OR'], self.ch + self.ch)
                else:
                    tok = self.new_token(TOKENS.tokens['ILLEGAL'], self.ch)
            # {{ Delimiters }}
            case ',':
                tok = self.new_token(TOKENS.tokens['COMMA'], self.ch)
            case ';':
                tok = self.new_token(TOKENS.tokens['SEMICOLON'], self.ch)
            case '(':
                tok = self.new_token(TOKENS.tokens['LPAREN'], self.ch)
            case ')':
                tok = self.new_token(TOKENS.tokens['RPAREN'], self.ch)
            case '{':
                tok = self.new_token(TOKENS.tokens['LBRACE'], self.ch)
            case '}':
                tok = self.new_token(TOKENS.tokens['RBRACE'], self.ch)
            case '"':
                self.__read_char()
                tok = self.new_token(TOKENS.tokens['STRING'], self.__read_identifier())
            # {{ Default Case }}
            case _:
                if self.__is_char(self.ch):
                    tok.Value = self.__read_identifier()
                    tok.Type = TOKENS.lookup_ident(tok.Value)
                    return tok
                elif self.__is_digit(self.ch):
                    tok.Type = TOKENS.tokens['INT']
                    tok.Value = self.__read_number()
                    return tok
                else:
                    tok = self.new_token(TOKENS.tokens['ILLEGAL'], self.ch)

        # Return a token depending on which character it is.
        self.__read_char()
        return tok
    

    # Returns the tokens for the input code.
    def get_tokens(self):
        tokens = []
        current_token = self.__next_token()
        tokens.append(tuple([current_token.Type, current_token.Value]))

        while current_token.Type != TOKENS.tokens['EOF']:
            current_token = self.__next_token()
            tokens.append(tuple([current_token.Type, current_token.Value]))

        return tokens
#+end_src

Most of the fields in Lexer are pretty self-explanatory. The ones that might cause some confusion right now are *position* and *next_position*. Both will be used to access characters in input by using them as an index, e.g.: *source[next_position]*. The reason for these two “pointers” pointing into our input string is the fact that we will need to be able to “peek” further into the input and look after the current character to see what comes up next. next_position always points to the “next” character in the input. position points to the character in the input that corresponds to the ch byte.

The purpose of *read_char* is to give us the next character and advance our position in the input string. The first thing it does is to check whether we have reached the end of input. If that’s the case it sets ch to 0, which is the *ASCII* code for the "NUL" character and signifies either “we haven’t read anything yet” or “end of file” for us. But if we haven’t reached the end of input yet it sets ch to the next character by accessing *source[next_position]*.

* RESULTS

#+begin_src python
from lexer import Lexer

def scan_file(file):
    f = open(file)

    my_lexer = Lexer(f.read())
    tokens = my_lexer.get_tokens()
    for tok in tokens:
        print(tok)

if __name__ == '__main__':
    scan_file('script.txt')
#+end_src
